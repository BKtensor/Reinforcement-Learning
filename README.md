# pygame-RL
use Rl play game made by pygame

english





中文

用Pygame做了一个小游戏，然后用不同的强化学习方法来玩。（model-free）
不同的方法，保存为不同的标题。

Q-learning，最简单却效果最好，最快速度收敛。所以说简单的问题，最好用简单的方法。不过这里是为了给后面的王者荣耀RL做准备，所以Q-learning的方法仅作为对照组。80回合左右形成稳定的最优策略。

DQN，一开始不调整replay顺序，很看脸，很容易完全跑不起来。随后加入了优先经验回放，训练变得稳定。
但是过估计的问题非常严重。在这个游戏中，我保留了撞墙的设定，因为后面做王者荣耀和暗黑的RL，都会有撞墙的情况。
结果过估计+撞墙，就会形成一个更加严重的过估计问题。
从数学上来看撞墙（或者原地不动）会让过估计得到继承。

DOUBLE DQN，用这个方法来减弱过估计，收敛更快。
但是得到最优策略以后，很快又会产生严重的过估计。EPSION降下来以后，智能体突然就在某个角落左右横跳。
如果长时间放着不管训练，回来一看基本都进入卡死在墙角的过估计状态中。

Dueling Double DQN，简称DDDQN,胜率稳定在100%，但是问题也很严重。
很明显他知道怎么不输，但他对怎么赢似乎不那么确定。后期老是来回横跳和撞墙，这个撞墙真的是DQN的老大难问题啊。

Policy Gradient,最原始的PG收敛速度较慢，不过没有那么严重的过估计问题。一般在500-600回合以后就能形成100%胜率。
注意学习率得低一点，然后要做一些reward shaping,不然这个难度很容易被负反馈统治。
现在我设置为0.0001的学习率，+10 -1的reward是比较稳定的。
不过退化问题依旧存在，如果不在最优状态停住，胜率坚持不了一会就会掉头往下。
不过因为没有严重的过估计，会重新收敛，形成一个治乱循环。



