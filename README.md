# pygame-RL
use Rl play game made by pygame

english





中文
用Pygame做了一个小游戏，然后用不同的强化学习方法来玩。

Q-learning，最简单却效果最好，最快速度收敛

DQN，一开始不调整replay顺序，很看脸，很容易完全跑不起来。随后加入了优先经验回放，训练变得稳定。
但是过估计的问题非常严重。在这个游戏中，我保留了撞墙的设定，因为后面做王者荣耀和暗黑的RL，都会有撞墙的情况。
结果过估计+撞墙，就会形成一个更加严重的过估计问题。我觉得撞墙对于过估计的加强，是后面做大型游戏RL时必须考虑的问题。

DOUBLE DQN，用这个方法来减弱过估计，收敛更快。
但是如果不在得到最优策略以后停止训练，很快又会产生严重的过估计。EPSION降下来以后，智能体突然就在某个角落左右横跳。
如果长时间放着不管训练，回来一看基本都进入卡死在墙角的过估计状态中。

Policy Gradient,最原始的PG收敛速度较慢，不过没有那么严重的过估计问题。一般在500-600回合以后就能形成100%胜率。
注意学习率得低一点，然后要做一些reward shaping,不然这个难度很容易被负反馈统治。
现在我设置为0.0001的学习率，+10 -1的reward是比较稳定的。
不过退化问题依旧存在，如果不在最优状态停住，胜率坚持不了一会就会掉头往下。
不过因为没有严重的过估计，会重新收敛，形成一个治乱循环。



