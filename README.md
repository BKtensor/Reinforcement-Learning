# use Reinforcement-Learning play game made by pygame

## english





## 中文

用Pygame做了一个小游戏，然后用不同的强化学习方法来玩。（model-free）
不同的方法，保存为不同的标题。

- [x] Q-learning，最简单却效果最好，最快速度收敛。所以说简单的问题，最好用简单的方法。不过这里是为了给后面的王者荣耀RL做准备，所以Q-learning的方法仅作为对照组。80回合左右形成稳定的最优策略。

- [x] DQN，一开始不调整replay顺序，很看脸，很容易完全跑不起来。随后加入了优先经验回放，训练变得稳定。
但是过估计的问题非常严重。在这个游戏中，我保留了撞墙的设定，因为后面做王者荣耀和暗黑的RL，都会有撞墙的情况。
结果过估计+撞墙，就会形成一个更加严重的过估计问题。
从数学上来看撞墙（或者原地不动）会让过估计多一个继承者，如果在角落就会多两个继承者。使得作为错误出现的过估计在角落割据成王了，不依靠随机选择的拯救，就很容易被墙角永远抓过去。所以轮次到后面，epsion加到0.95以上就经常卡死了。

- [x] DOUBLE DQN，用这个方法来减弱过估计，确实好了一些。但是过估计同样会出现，最终还是有可能卡死在墙角。只是减弱了症状，但不治本。

- [x] DDDQN,即Dueling Double DQN。350步以后收敛到最优策略，结果非常好，没有退化，没有观测到过估计问题。DQN之王

- [ ] multistep 3DQN。拖了很久，迟迟没做这个试验。因为觉得奖励过于稀疏，除了最后一步都是0，做多步更新也只是多用到几个为0的reward。如果效果提升，应该是随机小步数更新，能让接近终点的trace都用到最终reward。

----
- [x] Policy Gradient,最原始的PG收敛速度较慢，一般在500-600回合就能接近100%胜率。
注意学习率得低一点，然后要做一些reward shaping,不然这个难度很容易被负反馈统治。
现在我设置为1e-5的学习率，+10 -1的reward是比较稳定的。
有明显的退化问题，如果不在最优状态停住，胜率坚持不了一会就会掉头往下。
不过因为没有过估计，退化以后会重新收敛，形成一个治乱循环。


- [ ] Actor-Critic


- [ ] A3C

----

- [ ] state加入目的地位置和陷阱位置，位置每局随机
- [ ] 加入视觉，resnet处理画面代替state输入


